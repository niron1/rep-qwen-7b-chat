#!/usr/bin/env python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
CACHE_DIR = 'cache'

tokenizer = AutoTokenizer.from_pretrained(
    "vonjack/Qwen-LLaMAfied-HFTok-7B-Chat",
    use_cache=CACHE_DIR,
)
model = AutoModelForCausalLM.from_pretrained(
    "vonjack/Qwen-LLaMAfied-HFTok-7B-Chat",
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=False,
    use_cache=CACHE_DIR,
)
